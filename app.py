import sys
import os
import importlib.util
import streamlit as st

# --- 1. ç»å¯¹ç‰©ç†è·¯å¾„æ³¨å…¥ ---
# è¿™æ˜¯ä½ åˆšæ‰ find å‡ºæ¥çš„çœŸå®åæ ‡
ENSEMBLE_PATH = "/home/reusnak/neuro-symbolic-rag/.venv/lib/python3.12/site-packages/langchain_classic/retrievers/ensemble.py"
SITE_PACKAGES = "/home/reusnak/neuro-symbolic-rag/.venv/lib/python3.12/site-packages"

# å¼ºåˆ¶å°† site-packages åŠ å…¥æœç´¢è·¯å¾„
if SITE_PACKAGES not in sys.path:
    sys.path.insert(0, SITE_PACKAGES)

try:
    # æš´åŠ›åŠ è½½ï¼šç›´æ¥ä»ç‰©ç†æ–‡ä»¶è¯»å–
    spec = importlib.util.spec_from_file_location("ensemble_fixed", ENSEMBLE_PATH)
    ensemble_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(ensemble_module)
    EnsembleRetriever = ensemble_module.EnsembleRetriever
    print("âœ… ç‰©ç†åŠ è½½ EnsembleRetriever æˆåŠŸ")
except Exception as e:
    # å¦‚æœç‰©ç†åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ€åä¸€ç§æ ‡å‡†å¯¼å…¥
    from langchain.retrievers import EnsembleRetriever

from langchain_ollama import ChatOllama
try:
    # è·¯å¾„ A: æ–°ç‰ˆæœ¬çš„æ ‡å‡†ä½ç½®
    from langchain_core.messages import SystemMessage, HumanMessage
    print("âœ… é€šè¿‡ langchain_core åŠ è½½æ¶ˆæ¯ç»„ä»¶")
except ImportError:
    try:
        # è·¯å¾„ B: æŸäº›ç‰¹å®š 0.3.x ç‰ˆæœ¬çš„å…¼å®¹ä½ç½®
        from langchain.schema import SystemMessage, HumanMessage
    except ImportError:
        # è·¯å¾„ C: ç‰©ç†æ–‡ä»¶æš´åŠ›åŠ è½½ (æœ€åçš„ä¿åº•)
        import importlib.util
        # è¿™é‡Œçš„è·¯å¾„æ˜¯ 3.12 ç¯å¢ƒä¸‹çš„æ ‡å‡†æ ¸å¿ƒåŒ…ä½ç½®
        core_path = "/home/reusnak/neuro-symbolic-rag/.venv/lib/python3.12/site-packages/langchain_core/messages/__init__.py"
        if os.path.exists(core_path):
            spec = importlib.util.spec_from_file_location("messages_fixed", core_path)
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            SystemMessage = mod.SystemMessage
            HumanMessage = mod.HumanMessage
        else:
            st.error("âŒ æ— æ³•å®šä½ langchain_coreã€‚è¯·è¿è¡Œ: pip install langchain-core")
            st.stop()
from retriever import RAGRetriever
import config 

# --- 3. Streamlit é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Neuro-Symbolic RAG", 
    page_icon="ğŸ§ ", 
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ§  ç¥ç»ç¬¦å·çŸ¥è¯†åº“")
st.caption("åŸºäº Obsidian ç¬”è®°ã€å›¾è°±å¢å¼ºä¸æ··åˆæ£€ç´¢çš„æœ¬åœ° AI åŠ©æ‰‹")

# --- 4. èµ„æºåˆå§‹åŒ– (å¸¦ç¼“å­˜) ---
@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½ AI æ¨¡å‹ä¸ç´¢å¼•...")
def init_all():
    try:
        # åˆå§‹åŒ–æ£€ç´¢å¼•æ“
        engine = RAGRetriever()
        # åˆå§‹åŒ– LLM (Ollama)
        llm = ChatOllama(
            model=config.LLM_MODEL_NAME,
            base_url=config.OLLAMA_BASE_URL,
            temperature=0.3
        )
        return engine, llm
    except Exception as e:
        st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        st.info("è¯·ç¡®ä¿å·²é€šè¿‡ scripts/ingest.py æ„å»ºäº†ç´¢å¼•ï¼Œå¹¶å¯åŠ¨äº† Ollama æœåŠ¡ã€‚")
        st.stop()

engine, llm = init_all()

# --- 5. èŠå¤©è®°å½•ä¸ä¾§è¾¹æ  ---
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    st.header("âš™ï¸ ç³»ç»ŸçŠ¶æ€")
    st.success("âœ… æ£€ç´¢å¼•æ“: å°±ç»ª")
    st.info(f"ğŸ¤– å½“å‰æ¨¡å‹: {config.LLM_MODEL_NAME}")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# æ¸²æŸ“å¯¹è¯å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 6. æ ¸å¿ƒé—®ç­”é€»è¾‘ ---
if query := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # ç”ŸæˆåŠ©æ‰‹å›ç­”
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ‰§è¡Œæ£€ç´¢ (Retrieval)
            with st.status("ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...", expanded=False) as status:
                context = engine.search(query)
                status.update(label="âœ… æ£€ç´¢å®Œæˆ", state="complete")

            # ç¬¬äºŒæ­¥ï¼šæ„å»ºæ¶ˆæ¯åºåˆ—
            messages = [
                SystemMessage(content=f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Obsidian çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”é—®é¢˜ã€‚\n\nèƒŒæ™¯çŸ¥è¯†ï¼š\n{context}"),
                HumanMessage(content=query)
            ]

            # ç¬¬ä¸‰æ­¥ï¼šæµå¼ç”Ÿæˆ (Streaming)
            for chunk in llm.stream(messages):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"âš ï¸ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")