import torch
import pickle
import os
import networkx as nx
import sys
import importlib.util

# --- 1. ç»å¯¹ç‰©ç†è·¯å¾„æ³¨å…¥ (å¤„ç† langchain_classic å…¼å®¹æ€§) ---
ENSEMBLE_PATH = "/home/reusnak/neuro-symbolic-rag/.venv/lib/python3.12/site-packages/langchain_classic/retrievers/ensemble.py"
SITE_PACKAGES = "/home/reusnak/neuro-symbolic-rag/.venv/lib/python3.12/site-packages"

if SITE_PACKAGES not in sys.path:
    sys.path.insert(0, SITE_PACKAGES)

try:
    spec = importlib.util.spec_from_file_location("ensemble_fixed", ENSEMBLE_PATH)
    ensemble_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(ensemble_module)
    EnsembleRetriever = ensemble_module.EnsembleRetriever
except:
    from langchain.retrievers import EnsembleRetriever

from langchain_community.cross_encoders import HuggingFaceCrossEncoder
import config
from storage import StorageManager

class RAGRetriever:
    def __init__(self):
        print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡æ€æ£€ç´¢å¼•æ“...")
        self.storage = StorageManager()
        
        # è·å–æ£€ç´¢ç»„ä»¶ (vectorstore, docstore, bm25)
        components = self.storage.get_retriever_components()
        # --- å˜é‡åå¯¹é½ï¼šç¡®ä¿ä½¿ç”¨çš„æ˜¯ self.docstore ---
        self.vectorstore, self.docstore, self.bm25 = components
            
        self.graph = self.storage.load_graph()
        
        # 1. åˆå§‹åŒ– Reranker (ç²¾æ’æ¨¡å‹)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = os.path.basename(config.RERANKER_MODEL_PATH)
        print(f"ğŸ“¥ åŠ è½½é‡æ’åºæ¨¡å‹: {model_name} (è®¾å¤‡: {device})")
        
        try:
            self.reranker = HuggingFaceCrossEncoder(
                model_name=config.RERANKER_MODEL_PATH,
                model_kwargs={'device': device}
            )
        except Exception as e:
            print(f"âš ï¸ GPU åŠ è½½é‡æ’åºæ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ° CPU: {e}")
            self.reranker = HuggingFaceCrossEncoder(
                model_name=config.RERANKER_MODEL_PATH,
                model_kwargs={'device': 'cpu'}
            )

        # 2. ç»„åˆæ£€ç´¢å™¨ (æ··åˆå¬å›ï¼šå‘é‡ + å…³é”®è¯)
        if self.vectorstore:
            self.child_retriever = self.vectorstore.as_retriever(search_kwargs={"k": config.RETRIEVAL_K})
            
            if self.bm25:
                self.bm25.k = config.RETRIEVAL_K
                self.ensemble = EnsembleRetriever(
                    retrievers=[self.bm25, self.child_retriever], 
                    weights=[0.3, 0.7] 
                )
            else:
                print("âš ï¸ æœªå‘ç° BM25 ç´¢å¼•ï¼Œä»…ä½¿ç”¨å‘é‡æ£€ç´¢ã€‚")
                self.ensemble = self.child_retriever
        else:
            self.ensemble = None

    def _get_parent_content(self, child_docs):
        """è¿˜åŸçˆ¶æ–‡æ¡£ï¼šä½¿ç”¨ self.docstore (å¯¹é½ StorageManager)"""
        parent_ids = list({d.metadata.get("doc_id") for d in child_docs if "doc_id" in d.metadata})
        if not parent_ids: 
            return []
        
        # --- å…³é”®ä¿®æ”¹ï¼šç¡®ä¿å˜é‡åæ˜¯ docstore ---
        bytes_data = self.docstore.mget(parent_ids)
        return [pickle.loads(b) for b in bytes_data if b]

    def _graph_enhance(self, source_name, seen_sources):
        """å›¾è°±å¢å¼ºï¼šå¯»æ‰¾ Obsidian ä¸­çš„åŒé“¾å…³è”"""
        if self.graph is None or not self.graph.has_node(source_name): 
            return ""
        neighbors = [n for n in self.graph.neighbors(source_name) if n not in seen_sources]
        if not neighbors: 
            return ""
        return f"\n   [ğŸ’¡ å…³è”ç¬”è®°å»ºè®®]: {', '.join(neighbors[:3])}"

    def search(self, query):
        """æ ¸å¿ƒæ£€ç´¢æµç¨‹ï¼šæ··åˆå¬å› -> çˆ¶å—æ˜ å°„ -> é‡æ’åº -> å›¾è°±å¢å¼º"""
        if self.ensemble is None:
            return "âŒ ç³»ç»Ÿå°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ³¨å…¥è„šæœ¬ã€‚"

        # (1) æ··åˆå¬å›å­æ–‡æ¡£å—
        child_docs = self.ensemble.invoke(query)
        
        # (2) æ˜ å°„å›å…·æœ‰å®Œæ•´è¯­ä¹‰çš„çˆ¶æ–‡æ¡£
        parents = self._get_parent_content(child_docs)
        if not parents: 
            return "æœªæ‰¾åˆ°ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚"

        # (3) é‡æ’åº (Rerank)ï¼šè§£å†³ predict å±æ€§ä¸¢å¤±é—®é¢˜
        pairs = [[query, doc.page_content] for doc in parents]
        
        try:
            # å…¼å®¹æ€§è°ƒç”¨ï¼šLangChain 0.3+ å¯èƒ½ä¼šå°è£…åº•å±‚æ¨¡å‹
            if hasattr(self.reranker, 'model') and hasattr(self.reranker.model, 'predict'):
                scores = self.reranker.model.predict(pairs)
            elif hasattr(self.reranker, 'predict'):
                scores = self.reranker.predict(pairs)
            else:
                # æœ€åçš„ä¿åº•æ–¹æ¡ˆ
                print("âš ï¸ æ— æ³•åœ¨ Reranker ä¸Šæ‰¾åˆ° predict æ–¹æ³•ï¼Œå°è¯•ç›´æ¥è°ƒç”¨åº•å±‚çš„ client")
                scores = self.reranker.client.predict(pairs)
        except Exception as e:
            print(f"âš ï¸ é‡æ’åºå¤±è´¥: {e}ï¼Œå°†æŒ‰åŸå§‹é¡ºåºæ’åˆ—")
            scores = [1.0] * len(parents)

        ranked = sorted(zip(parents, scores), key=lambda x: x[1], reverse=True)
        top_docs = [doc for doc, score in ranked[:config.RERANK_TOP_K]]

        # (4) ç»„è£…æœ€ç»ˆä¸Šä¸‹æ–‡
        context_parts = []
        seen_sources = set()
        for doc in top_docs:
            src = doc.metadata.get("source", "æœªçŸ¥")
            seen_sources.add(src)
            
            h1 = doc.metadata.get("H1", "")
            h2 = doc.metadata.get("H2", "")
            path_info = f" -> {h1}" if h1 else ""
            if h2: path_info += f" -> {h2}"

            header = f"ã€æ¥æº: {src}{path_info}ã€‘"
            graph_info = self._graph_enhance(src, seen_sources)
            
            context_parts.append(f"{header}\n{doc.page_content}{graph_info}")

        return "\n\n".join(context_parts)